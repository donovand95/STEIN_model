{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as math\n",
    "from scipy.interpolate import interp1d, interp2d\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import random\n",
    "from IPython.display import display, clear_output\n",
    "from jupyterthemes import jtplot\n",
    "import time\n",
    "import scipy as scipy\n",
    "import csv\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [],
   "source": [
    "TrainDataDictionary = {'total_time': 17500, \n",
    "     'scenarios': 10, \n",
    "     'schemes': 6,\n",
    "     'sample_no': 30,\n",
    "     'rock_rho': 2.62,\n",
    "     'SurfBe10': 3.9967450849710744,\n",
    "     'SurfC14': 15.776887273257842,\n",
    "     'SurfHe3': 128.0,\n",
    "     'CN_lambda': 160,\n",
    "     'Be10DecayCon': np.log(2)/1.39e6}\n",
    "\n",
    "TestDataDictionary = {'total_time': 17500, \n",
    "     'scenarios': 12, \n",
    "     'schemes': 2,\n",
    "     'sample_no': 30,\n",
    "     'rock_rho': 2.62,\n",
    "     'SurfBe10': 3.9967450849710744,\n",
    "     'SurfC14': 15.776887273257842,\n",
    "     'SurfHe3': 128.0,\n",
    "     'CN_lambda': 160,\n",
    "     'Be10DecayCon': np.log(2)/1.39e6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: 'D:\\\\STEIN_paper\\\\model_outputs\\\\scripts\\\\for_publication # set this equal to the folder containing the data download'\n",
      "C:\\Users\\dennis\\Documents\\STEIN_links\\for_publication\n"
     ]
    }
   ],
   "source": [
    "%pwd                                                      # print your current working directory\n",
    "%cd D:\\STEIN_paper\\model_outputs\\scripts\\for_publication  # set this equal to the folder containing the data download\n",
    "%run manuscript_data_processing_fxns.ipynb                # run script containing processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of (meaningless) erosion rates used to import constant arrays\n",
    "\n",
    "ListMeasured = pd.DataFrame(np.array([0.04945600175882841, 0.09791967954754568, 0.24580043601119, 0.09840629477594534, 0.105188042226475850, 0.09866676939027233]),\n",
    "                            index = ['EVAL ER0.05', 'EVAL ER0.10', 'EVAL ER0.25','EVAL c0.5', 'EVAL c0.7','EVAL c1.0'],\n",
    "                            columns = ['import summary value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a table of erosion rates for the evaluation data set\n",
    "\n",
    "EvalDataERLists = pd.read_csv('EVAL_erosion_rates_to_import.csv', sep = ';',\n",
    "                          index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "locals().update(TrainDataDictionary)      # set relevant values for evaluation dataset calculations\n",
    "SpunUp = int(total_time *0.1)             # uniform spin-up time so early concentrations are not included (determined to be sufficiently long through prior tests )\n",
    "\n",
    "EvalDataImported = CreateMainArrays(total_time,scenarios,sample_no,schemes)\n",
    "EvalDataMain = EvalDataImported[0]\n",
    "EvalDataMeasured = EvalDataImported[1]\n",
    "EvalDataMean = EvalDataImported[2]\n",
    "\n",
    "EvalActualERs = CalculateActualERs(EvalDataMain, scenarios, schemes, total_time).set_index(EvalDataERLists.columns)\n",
    "EvalMeasERs = CalculateMeasERs(EvalDataMeasured, scenarios, schemes)\n",
    "EvalSampling1000 = GenerateSamples(scenarios,schemes,sample_no,EvalDataMain, total_time)\n",
    "EvalSamplingCalculations = SampleCalculations(scenarios,schemes,sample_no,EvalSampling1000,EvalDataMean, EvalActualERs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "locals().update(TestDataDictionary)  #  set relevant values for test dataset calculations\n",
    "\n",
    "TestDataImported = CreateTestArrays(total_time, scenarios, sample_no, schemes)\n",
    "TestDataMain = TestDataImported[0]\n",
    "TestDataMeasured = TestDataImported[1]\n",
    "TestDataMean = TestDataImported[2]\n",
    "\n",
    "TestDataActualERs = CalculateActualERs(TestDataMain, scenarios, schemes, total_time)\n",
    "TestDataMeasERs = CalculateMeasERsTestData(TestDataMeasured, scenarios, schemes)\n",
    "TestDataSampling1000 = GenerateSamples(scenarios,schemes,sample_no,TestDataMain, total_time)\n",
    "TestDataSamplingCalculations = SampleCalculationsTestData(scenarios,schemes,sample_no,TestDataSampling1000,TestDataMean,TestDataActualERs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
